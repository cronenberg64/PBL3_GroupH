{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Network Training for Cat Identification\n",
    "\n",
    "This notebook contains the complete pipeline for training a Siamese network to identify individual cats from images. The process includes:\n",
    "\n",
    "1.  **Dependency Checking**: Ensuring all required libraries are installed.\n",
    "2.  **Dataset Analysis**: Exploring the dataset's structure and statistics.\n",
    "3.  **Data Preparation**: Loading images and creating pairs/triplets for training.\n",
    "4.  **Model Building**: Defining the Siamese network architecture with a pre-trained base model.\n",
    "5.  **Training**: Training the model using both Contrastive and Triplet loss functions.\n",
    "6.  **Evaluation**: Assessing the performance of the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data processing and ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import EfficientNetB0, VGG16, MobileNetV2\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dependency Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dependencies():\n",
    "    \"\"\"Check if all required dependencies are installed\"\"\"\n",
    "    print(\"Checking dependencies...\")\n",
    "\n",
    "    # Map package names to their import names\n",
    "    package_imports = {\n",
    "        'tensorflow': 'tensorflow',\n",
    "        'numpy': 'numpy',\n",
    "        'pandas': 'pandas',\n",
    "        'matplotlib': 'matplotlib',\n",
    "        'seaborn': 'seaborn',\n",
    "        'scikit-learn': 'sklearn',\n",
    "        'opencv-python': 'cv2',\n",
    "        'Pillow': 'PIL',\n",
    "        'tqdm': 'tqdm'\n",
    "    }\n",
    "\n",
    "    missing_packages = []\n",
    "    for package, import_name in package_imports.items():\n",
    "        try:\n",
    "            __import__(import_name)\n",
    "            print(f\"✓ {package}\")\n",
    "        except ImportError:\n",
    "            missing_packages.append(package)\n",
    "            print(f\"✗ {package} - MISSING\")\n",
    "\n",
    "    if missing_packages:\n",
    "        print(f\"\\nMissing packages: {', '.join(missing_packages)}\")\n",
    "        print(\"Please install them using:\")\n",
    "        print(f\"pip install {' '.join(missing_packages)}\")\n",
    "        return False\n",
    "\n",
    "    print(\"\\nAll dependencies are installed!\")\n",
    "    return True\n",
    "\n",
    "check_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset_path):\n",
    "    \"\"\"Analyze the structure and statistics of the dataset\"\"\"\n",
    "    print(\"Analyzing dataset structure...\")\n",
    "    \n",
    "    # Get all cat folders\n",
    "    cat_folders = [f for f in os.listdir(dataset_path) \n",
    "                  if f.startswith('cat_') and os.path.isdir(os.path.join(dataset_path, f))]\n",
    "    cat_folders.sort()\n",
    "    \n",
    "    print(f\"Found {len(cat_folders)} cat folders\")\n",
    "    \n",
    "    # Analyze each cat folder\n",
    "    cat_stats = []\n",
    "    total_images = 0\n",
    "    image_extensions = Counter()\n",
    "    \n",
    "    for cat_folder in cat_folders:\n",
    "        cat_path = os.path.join(dataset_path, cat_folder)\n",
    "        \n",
    "        # Count images by extension\n",
    "        image_files = [f for f in os.listdir(cat_path) \n",
    "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        extensions = [os.path.splitext(f)[1].lower() for f in image_files]\n",
    "        image_extensions.update(extensions)\n",
    "        \n",
    "        # Check if info.json exists\n",
    "        info_file = os.path.join(cat_path, 'info.json')\n",
    "        has_info = os.path.exists(info_file)\n",
    "        \n",
    "        cat_stats.append({\n",
    "            'cat_id': cat_folder,\n",
    "            'num_images': len(image_files),\n",
    "            'has_info': has_info,\n",
    "            'extensions': list(set(extensions))\n",
    "        })\n",
    "        \n",
    "        total_images += len(image_files)\n",
    "    \n",
    "    # Create summary\n",
    "    summary = {\n",
    "        'total_cats': len(cat_folders),\n",
    "        'total_images': total_images,\n",
    "        'avg_images_per_cat': total_images / len(cat_folders) if cat_folders else 0,\n",
    "        'image_extensions': dict(image_extensions),\n",
    "        'cats_with_info': sum(1 for cat in cat_stats if cat['has_info']),\n",
    "        'cats_without_info': sum(1 for cat in cat_stats if not cat['has_info'])\n",
    "    }\n",
    "    \n",
    "    return cat_stats, summary\n",
    "\n",
    "def visualize_dataset_stats(cat_stats, summary):\n",
    "    \"\"\"Create visualizations of dataset statistics\"\"\"\n",
    "    print(\"\\nCreating dataset visualizations...\")\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    df = pd.DataFrame(cat_stats)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Distribution of images per cat\n",
    "    ax1.hist(df['num_images'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.set_xlabel('Number of Images per Cat')\n",
    "    ax1.set_ylabel('Number of Cats')\n",
    "    ax1.set_title('Distribution of Images per Cat')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Image count by cat (top 20)\n",
    "    top_cats = df.nlargest(20, 'num_images')\n",
    "    ax2.barh(range(len(top_cats)), top_cats['num_images'], color='lightcoral')\n",
    "    ax2.set_yticks(range(len(top_cats)))\n",
    "    ax2.set_yticklabels([cat_id[:15] + '...' if len(cat_id) > 15 else cat_id for cat_id in top_cats['cat_id']])\n",
    "    ax2.set_xlabel('Number of Images')\n",
    "    ax2.set_title('Top 20 Cats by Image Count')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Image extensions distribution\n",
    "    extensions = summary['image_extensions']\n",
    "    ax3.pie(extensions.values(), labels=extensions.keys(), autopct='%1.1f%%')\n",
    "    ax3.set_title('Distribution of Image File Types')\n",
    "    \n",
    "    # 4. Info.json availability\n",
    "    info_counts = [summary['cats_with_info'], summary['cats_without_info']]\n",
    "    info_labels = ['With Info', 'Without Info']\n",
    "    ax4.bar(info_labels, info_counts, color=['lightgreen', 'lightcoral'])\n",
    "    ax4.set_ylabel('Number of Cats')\n",
    "    ax4.set_title('Info.json Availability')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dataset_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "dataset_path = 'post_processing' # Changed from 'siamese_dataset'\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"Dataset path '{dataset_path}' not found!\")\n",
    "else:\n",
    "    cat_stats, summary = analyze_dataset(dataset_path)\n",
    "    print(\"\\nDataset Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total cats: {summary['total_cats']}\")\n",
    "    print(f\"Total images: {summary['total_images']}\")\n",
    "    print(f\"Average images per cat: {summary['avg_images_per_cat']:.1f}\")\n",
    "    print(f\"Image extensions: {summary['image_extensions']}\")\n",
    "    print(f\"Cats with info.json: {summary['cats_with_info']}\")\n",
    "    print(f\"Cats without info.json: {summary['cats_without_info']}\")\n",
    "    df_stats = visualize_dataset_stats(cat_stats, summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 128\n",
    "MARGIN = 1.0\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1\n",
    "DATASET_PATH = 'post_processing' # Changed from 'siamese_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataset:\n",
    "    def __init__(self, dataset_path, img_size=IMG_SIZE):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.img_size = img_size\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def load_dataset(self, max_cats=None, min_images_per_cat=5):\n",
    "        \"\"\"Load images from the organized dataset\"\"\"\n",
    "        print(\"Loading dataset...\")\n",
    "        \n",
    "        cat_folders = [f for f in os.listdir(self.dataset_path) \n",
    "                      if f.startswith('cat_') and os.path.isdir(os.path.join(self.dataset_path, f))]\n",
    "        cat_folders.sort()\n",
    "        \n",
    "        if max_cats:\n",
    "            cat_folders = cat_folders[:max_cats]\n",
    "        \n",
    "        print(f\"Found {len(cat_folders)} cat folders\")\n",
    "        \n",
    "        for cat_folder in tqdm(cat_folders, desc=\"Loading cats\"):\n",
    "            cat_path = os.path.join(self.dataset_path, cat_folder)\n",
    "            image_files = [f for f in os.listdir(cat_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            \n",
    "            if len(image_files) < min_images_per_cat:\n",
    "                print(f\"Skipping {cat_folder} - only {len(image_files)} images\")\n",
    "                continue\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                img_path = os.path.join(cat_path, img_file)\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "                        img = img.astype(np.float32) / 255.0\n",
    "                        self.images.append(img)\n",
    "                        self.labels.append(cat_folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {img_path}: {e}\")\n",
    "        \n",
    "        self.images = np.array(self.images)\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.encoded_labels = self.label_encoder.fit_transform(self.labels)\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images from {len(np.unique(self.labels))} cats\")\n",
    "        print(f\"Image shape: {self.images.shape}\")\n",
    "        \n",
    "        return self.images, self.encoded_labels\n",
    "    \n",
    "    def create_pairs(self, images, labels, num_pairs_per_image=2):\n",
    "        \"\"\"Create positive and negative pairs for training\"\"\"\n",
    "        print(\"Creating training pairs...\")\n",
    "        pair_images = []\n",
    "        pair_labels = []\n",
    "        unique_labels = np.unique(labels)\n",
    "        label_indices = {label: np.where(labels == label)[0] for label in unique_labels}\n",
    "        \n",
    "        for i in tqdm(range(len(images)), desc=\"Creating pairs\"):\n",
    "            current_image = images[i]\n",
    "            current_label = labels[i]\n",
    "            \n",
    "            # Positive pairs\n",
    "            pos_indices = label_indices[current_label]\n",
    "            for _ in range(num_pairs_per_image):\n",
    "                pos_idx = random.choice(pos_indices)\n",
    "                if pos_idx != i:\n",
    "                    pos_image = images[pos_idx]\n",
    "                    pair_images.append([current_image, pos_image])\n",
    "                    pair_labels.append(0)\n",
    "            \n",
    "            # Negative pairs\n",
    "            neg_labels = [l for l in unique_labels if l != current_label]\n",
    "            for _ in range(num_pairs_per_image):\n",
    "                neg_label = random.choice(neg_labels)\n",
    "                neg_idx = random.choice(label_indices[neg_label])\n",
    "                neg_image = images[neg_idx]\n",
    "                pair_images.append([current_image, neg_image])\n",
    "                pair_labels.append(1)\n",
    "        \n",
    "        return np.array(pair_images), np.array(pair_labels)\n",
    "    \n",
    "    def create_triplets(self, images, labels, num_triplets_per_image=1):\n",
    "        \"\"\"Create triplets for triplet loss training\"\"\"\n",
    "        print(\"Creating training triplets...\")\n",
    "        anchor_images, positive_images, negative_images = [], [], []\n",
    "        unique_labels = np.unique(labels)\n",
    "        label_indices = {label: np.where(labels == label)[0] for label in unique_labels}\n",
    "        \n",
    "        for i in tqdm(range(len(images)), desc=\"Creating triplets\"):\n",
    "            anchor_image = images[i]\n",
    "            anchor_label = labels[i]\n",
    "            \n",
    "            for _ in range(num_triplets_per_image):\n",
    "                pos_indices = label_indices[anchor_label]\n",
    "                pos_idx = random.choice(pos_indices)\n",
    "                if pos_idx != i:\n",
    "                    positive_image = images[pos_idx]\n",
    "                    neg_labels = [l for l in unique_labels if l != anchor_label]\n",
    "                    neg_label = random.choice(neg_labels)\n",
    "                    neg_idx = random.choice(label_indices[neg_label])\n",
    "                    negative_image = images[neg_idx]\n",
    "                    anchor_images.append(anchor_image)\n",
    "                    positive_images.append(positive_image)\n",
    "                    negative_images.append(negative_image)\n",
    "        \n",
    "        return (np.array(anchor_images), np.array(positive_images), np.array(negative_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel:\n",
    "    def __init__(self, input_shape, embedding_dim=EMBEDDING_DIM, base_model='efficientnet'):\n",
    "        self.input_shape = input_shape\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.base_model_name = base_model\n",
    "        \n",
    "    def create_embedding_model(self):\n",
    "        \"\"\"Create the base embedding model\"\"\"\n",
    "        if self.base_model_name == 'efficientnet':\n",
    "            base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=self.input_shape)\n",
    "        elif self.base_model_name == 'vgg':\n",
    "            base_model = VGG16(include_top=False, weights='imagenet', input_shape=self.input_shape)\n",
    "        elif self.base_model_name == 'mobilenet':\n",
    "            base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=self.input_shape)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported base model: {self.base_model_name}\")\n",
    "        \n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        x = base_model(inputs)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(self.embedding_dim, activation=None)(x)\n",
    "        \n",
    "        return Model(inputs, outputs, name='embedding_model')\n",
    "    \n",
    "    def create_siamese_model(self, loss_type='contrastive'):\n",
    "        \"\"\"Create the complete Siamese model\"\"\"\n",
    "        embedding_model = self.create_embedding_model()\n",
    "        \n",
    "        if loss_type == 'contrastive':\n",
    "            return self._create_contrastive_model(embedding_model)\n",
    "        elif loss_type == 'triplet':\n",
    "            return self._create_triplet_model(embedding_model)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
    "    \n",
    "    def _create_contrastive_model(self, embedding_model):\n",
    "        input_a = Input(shape=self.input_shape)\n",
    "        input_b = Input(shape=self.input_shape)\n",
    "        embedding_a = embedding_model(input_a)\n",
    "        embedding_b = embedding_model(input_b)\n",
    "        distance = self._euclidean_distance([embedding_a, embedding_b])\n",
    "        model = Model(inputs=[input_a, input_b], outputs=distance)\n",
    "        model.compile(loss=self._contrastive_loss, optimizer=Adam(learning_rate=LEARNING_RATE), metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def _create_triplet_model(self, embedding_model):\n",
    "        anchor_input = Input(shape=self.input_shape)\n",
    "        positive_input = Input(shape=self.input_shape)\n",
    "        negative_input = Input(shape=self.input_shape)\n",
    "        anchor_embedding = embedding_model(anchor_input)\n",
    "        positive_embedding = embedding_model(positive_input)\n",
    "        negative_embedding = embedding_model(negative_input)\n",
    "        loss = self._triplet_loss([anchor_embedding, positive_embedding, negative_embedding])\n",
    "        model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=loss)\n",
    "        model.compile(loss=self._identity_loss, optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "        return model\n",
    "    \n",
    "    def _euclidean_distance(self, vectors):\n",
    "        (feats_a, feats_b) = vectors\n",
    "        sum_squared = tf.reduce_sum(tf.square(feats_a - feats_b), axis=1, keepdims=True)\n",
    "        return tf.sqrt(tf.maximum(sum_squared, tf.keras.backend.epsilon()))\n",
    "    \n",
    "    def _contrastive_loss(self, y_true, y_pred, margin=MARGIN):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        squared_preds = tf.square(y_pred)\n",
    "        squared_margin = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "        loss = tf.reduce_mean((1 - y_true) * squared_preds + y_true * squared_margin)\n",
    "        return loss\n",
    "    \n",
    "    def _triplet_loss(self, inputs, alpha=0.2):\n",
    "        anchor, positive, negative = inputs\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "        basic_loss = pos_dist - neg_dist + alpha\n",
    "        loss = tf.maximum(basic_loss, 0.0)\n",
    "        return loss\n",
    "    \n",
    "    def _identity_loss(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseTrainer:\n",
    "    def __init__(self, dataset, model, loss_type='contrastive'):\n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        self.loss_type = loss_type\n",
    "        self.history = None\n",
    "        \n",
    "    def train(self, train_data, val_data, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
    "        print(f\"\\nTraining Siamese model with {self.loss_type} loss...\")\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(f'best_siamese_{self.loss_type}.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n",
    "        ]\n",
    "        \n",
    "        if self.loss_type == 'contrastive':\n",
    "            self.history = self.model.fit(\n",
    "                [train_data[0][:, 0], train_data[0][:, 1]], train_data[1],\n",
    "                validation_data=([val_data[0][:, 0], val_data[0][:, 1]], val_data[1]),\n",
    "                batch_size=batch_size, epochs=epochs, callbacks=callbacks, verbose=1\n",
    "            )\n",
    "        elif self.loss_type == 'triplet':\n",
    "            self.history = self.model.fit(\n",
    "                [train_data[0], train_data[1], train_data[2]], np.ones(len(train_data[0])),\n",
    "                validation_data=([val_data[0], val_data[1], val_data[2]], np.ones(len(val_data[0]))),\n",
    "                batch_size=batch_size, epochs=epochs, callbacks=callbacks, verbose=1\n",
    "            )\n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self, test_data, test_labels):\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        embedding_model = self.model.layers[2]\n",
    "        test_embeddings = embedding_model.predict(test_data)\n",
    "        \n",
    "        predictions = []\n",
    "        for i, embedding in enumerate(test_embeddings):\n",
    "            distances = []\n",
    "            for j, other_embedding in enumerate(test_embeddings):\n",
    "                if i != j:\n",
    "                    dist = np.linalg.norm(embedding - other_embedding)\n",
    "                    distances.append((dist, test_labels[j]))\n",
    "            distances.sort(key=lambda x: x[0])\n",
    "            predictions.append(distances[0][1])\n",
    "        \n",
    "        accuracy = accuracy_score(test_labels, predictions)\n",
    "        precision = precision_score(test_labels, predictions, average='weighted')\n",
    "        recall = recall_score(test_labels, predictions, average='weighted')\n",
    "        f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "        \n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Test Precision: {precision:.4f}\")\n",
    "        print(f\"Test Recall: {recall:.4f}\")\n",
    "        print(f\"Test F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1}\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        if self.history is None: return\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        ax1.plot(self.history.history['loss'], label='Training Loss')\n",
    "        ax1.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        ax1.set_title('Model Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        if 'accuracy' in self.history.history:\n",
    "            ax2.plot(self.history.history['accuracy'], label='Training Accuracy')\n",
    "            ax2.plot(self.history.history['val_accuracy'], label='Validation Accuracy')\n",
    "            ax2.set_title('Model Accuracy')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'training_history_{self.loss_type}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preparation\n",
    "print(\"Step 1: Data Preparation\")\n",
    "dataset = SiameseDataset(DATASET_PATH, img_size=IMG_SIZE)\n",
    "images, labels = dataset.load_dataset(max_cats=20, min_images_per_cat=5)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(images, labels, test_size=TEST_SPLIT, stratify=labels, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=VALIDATION_SPLIT, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} images\")\n",
    "print(f\"Validation set: {len(X_val)} images\")\n",
    "print(f\"Test set: {len(X_test)} images\")\n",
    "\n",
    "# Step 2: Model Architecture\n",
    "print(\"\\nStep 2: Model Architecture\")\n",
    "siamese_model_builder = SiameseModel(input_shape=(IMG_SIZE, IMG_SIZE, 3), embedding_dim=EMBEDDING_DIM, base_model='efficientnet')\n",
    "\n",
    "# Step 3: Training Pipeline\n",
    "print(\"\\nStep 3: Training Pipeline\")\n",
    "\n",
    "# Train with contrastive loss\n",
    "contrastive_model = siamese_model_builder.create_siamese_model(loss_type='contrastive')\n",
    "train_pairs, train_pair_labels = dataset.create_pairs(X_train, y_train)\n",
    "val_pairs, val_pair_labels = dataset.create_pairs(X_val, y_val)\n",
    "contrastive_trainer = SiameseTrainer(dataset, contrastive_model, loss_type='contrastive')\n",
    "contrastive_history = contrastive_trainer.train((train_pairs, train_pair_labels), (val_pairs, val_pair_labels))\n",
    "contrastive_metrics = contrastive_trainer.evaluate(X_test, y_test)\n",
    "contrastive_trainer.plot_training_history()\n",
    "\n",
    "# Train with triplet loss\n",
    "triplet_model = siamese_model_builder.create_siamese_model(loss_type='triplet')\n",
    "train_triplets = dataset.create_triplets(X_train, y_train)\n",
    "val_triplets = dataset.create_triplets(X_val, y_val)\n",
    "triplet_trainer = SiameseTrainer(dataset, triplet_model, loss_type='triplet')\n",
    "triplet_history = triplet_trainer.train(train_triplets, val_triplets)\n",
    "triplet_metrics = triplet_trainer.evaluate(X_test, y_test)\n",
    "triplet_trainer.plot_training_history()\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame([\n",
    "    {'loss_type': 'contrastive', **contrastive_metrics},\n",
    "    {'loss_type': 'triplet', **triplet_metrics}\n",
    "])\n",
    "results_df.to_csv('siamese_training_results.csv', index=False)\n",
    "print(f\"\\nResults saved to siamese_training_results.csv\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Contrastive Loss Results:\")\n",
    "print(results_df[results_df['loss_type'] == 'contrastive'])\n",
    "print(\"\\nTriplet Loss Results:\")\n",
    "print(results_df[results_df['loss_type'] == 'triplet'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
